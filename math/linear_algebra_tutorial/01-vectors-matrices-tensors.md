# 1. Advanced Vectors, Matrices, and Tensors: Mathematical Foundations

## Introduction

This advanced tutorial provides a rigorous mathematical foundation for vectors, matrices, and tensors, establishing the theoretical framework that underlies all of machine learning. We'll move beyond computational aspects to understand the deep mathematical structures and their properties.

## Mathematical Preliminaries

### Field Theory and Vector Spaces

A **field** is a set F with two operations (+, Ã—) satisfying:
- **Commutativity**: a + b = b + a, a Ã— b = b Ã— a
- **Associativity**: (a + b) + c = a + (b + c), (a Ã— b) Ã— c = a Ã— (b Ã— c)
- **Distributivity**: a Ã— (b + c) = a Ã— b + a Ã— c
- **Identity elements**: 0 + a = a, 1 Ã— a = a
- **Inverses**: For every a, there exists -a and aâ»Â¹ (if a â‰  0)

**Common fields**: â„ (real numbers), â„‚ (complex numbers), â„š (rational numbers), ð”½â‚š (finite field of order p)

### Vector Space Definition

A **vector space** V over a field F is a set with:
1. **Vector addition**: + : V Ã— V â†’ V
2. **Scalar multiplication**: Â· : F Ã— V â†’ V

Satisfying the following axioms:

**Vector Addition Axioms:**
- **Commutativity**: v + w = w + v
- **Associativity**: (v + w) + u = v + (w + u)
- **Identity**: âˆƒ0 âˆˆ V such that v + 0 = v
- **Inverse**: âˆ€v âˆˆ V, âˆƒ(-v) âˆˆ V such that v + (-v) = 0

**Scalar Multiplication Axioms:**
- **Distributivity**: a(v + w) = av + aw
- **Distributivity**: (a + b)v = av + bv
- **Associativity**: a(bv) = (ab)v
- **Identity**: 1v = v

## Advanced Vector Theory

### Vector Space Examples

```python
import numpy as np
from typing import List, Union, Tuple
import sympy as sp

class VectorSpace:
    """Abstract vector space implementation"""
    
    def __init__(self, field: str = "real"):
        self.field = field
        if field == "real":
            self.scalar_type = float
        elif field == "complex":
            self.scalar_type = complex
        else:
            raise ValueError(f"Unsupported field: {field}")
    
    def validate_vector(self, v: np.ndarray) -> bool:
        """Validate if vector belongs to this space"""
        return v.dtype == self.scalar_type
    
    def add_vectors(self, v1: np.ndarray, v2: np.ndarray) -> np.ndarray:
        """Vector addition with validation"""
        if not (self.validate_vector(v1) and self.validate_vector(v2)):
            raise ValueError("Vectors must belong to the same space")
        if v1.shape != v2.shape:
            raise ValueError("Vectors must have the same shape")
        return v1 + v2
    
    def scalar_multiply(self, scalar: Union[float, complex], v: np.ndarray) -> np.ndarray:
        """Scalar multiplication with validation"""
        if not isinstance(scalar, self.scalar_type):
            raise ValueError(f"Scalar must be of type {self.scalar_type}")
        return scalar * v

# Example usage
real_space = VectorSpace("real")
complex_space = VectorSpace("complex")

v1 = np.array([1.0, 2.0, 3.0])
v2 = np.array([4.0, 5.0, 6.0])

# Vector addition
v_sum = real_space.add_vectors(v1, v2)
print(f"Vector sum: {v_sum}")

# Scalar multiplication
scaled_v = real_space.scalar_multiply(2.5, v1)
print(f"Scaled vector: {scaled_v}")
```

### Linear Independence and Basis

**Definition**: A set of vectors {vâ‚, vâ‚‚, ..., vâ‚™} is **linearly independent** if:
```
câ‚vâ‚ + câ‚‚vâ‚‚ + ... + câ‚™vâ‚™ = 0 âŸ¹ câ‚ = câ‚‚ = ... = câ‚™ = 0
```

**Definition**: A **basis** for a vector space V is a linearly independent set that spans V.

**Theorem**: Every vector space has a basis, and all bases have the same cardinality (dimension).

```python
def check_linear_independence(vectors: List[np.ndarray]) -> bool:
    """Check if vectors are linearly independent using matrix rank"""
    # Stack vectors into a matrix
    matrix = np.column_stack(vectors)
    
    # Check if rank equals number of vectors
    rank = np.linalg.matrix_rank(matrix)
    return rank == len(vectors)

def find_basis(vectors: List[np.ndarray]) -> List[np.ndarray]:
    """Find a basis for the span of given vectors"""
    matrix = np.column_stack(vectors)
    
    # Use QR decomposition to find linearly independent columns
    Q, R, P = sp.Matrix(matrix).QRdecomposition()
    
    # Extract basis vectors
    basis_indices = []
    for i, row in enumerate(R.tolist()):
        if any(abs(val) > 1e-10 for val in row):
            basis_indices.append(P[i])
    
    return [vectors[i] for i in basis_indices]

# Example
vectors = [
    np.array([1, 0, 0]),
    np.array([0, 1, 0]),
    np.array([1, 1, 0]),
    np.array([0, 0, 1])
]

print(f"Vectors are linearly independent: {check_linear_independence(vectors)}")
basis = find_basis(vectors)
print(f"Basis vectors: {len(basis)}")
```

### Inner Product Spaces

**Definition**: An **inner product** on a vector space V is a function âŸ¨Â·, Â·âŸ© : V Ã— V â†’ F satisfying:
1. **Conjugate symmetry**: âŸ¨v, wâŸ© = âŸ¨w, vâŸ©*
2. **Linearity**: âŸ¨av + bw, uâŸ© = aâŸ¨v, uâŸ© + bâŸ¨w, uâŸ©
3. **Positive definiteness**: âŸ¨v, vâŸ© â‰¥ 0 and âŸ¨v, vâŸ© = 0 âŸ¹ v = 0

**Common inner products:**
- **Euclidean**: âŸ¨v, wâŸ© = vâ‚wâ‚ + vâ‚‚wâ‚‚ + ... + vâ‚™wâ‚™
- **Complex**: âŸ¨v, wâŸ© = vâ‚*wâ‚ + vâ‚‚*wâ‚‚ + ... + vâ‚™*wâ‚™
- **Weighted**: âŸ¨v, wâŸ© = vâ‚wâ‚/Ïƒâ‚Â² + vâ‚‚wâ‚‚/Ïƒâ‚‚Â² + ... + vâ‚™wâ‚™/Ïƒâ‚™Â²

```python
class InnerProductSpace(VectorSpace):
    """Vector space with inner product structure"""
    
    def __init__(self, field: str = "real", metric: np.ndarray = None):
        super().__init__(field)
        self.metric = metric if metric is not None else np.eye(3)
    
    def inner_product(self, v1: np.ndarray, v2: np.ndarray) -> Union[float, complex]:
        """Compute inner product with metric tensor"""
        if self.field == "real":
            return v1.T @ self.metric @ v2
        else:
            return v1.conj().T @ self.metric @ v2
    
    def norm(self, v: np.ndarray) -> float:
        """Compute vector norm"""
        return np.sqrt(np.real(self.inner_product(v, v)))
    
    def angle(self, v1: np.ndarray, v2: np.ndarray) -> float:
        """Compute angle between vectors"""
        cos_angle = self.inner_product(v1, v2) / (self.norm(v1) * self.norm(v2))
        return np.arccos(np.clip(cos_angle, -1, 1))
    
    def orthogonalize(self, vectors: List[np.ndarray]) -> List[np.ndarray]:
        """Gram-Schmidt orthogonalization with inner product"""
        orthogonal_vectors = []
        
        for v in vectors:
            v_orth = v.copy()
            
            # Subtract projections onto previous vectors
            for u in orthogonal_vectors:
                proj_coeff = self.inner_product(v, u) / self.inner_product(u, u)
                v_orth = v_orth - proj_coeff * u
            
            # Normalize
            norm_v = self.norm(v_orth)
            if norm_v > 1e-10:
                orthogonal_vectors.append(v_orth / norm_v)
        
        return orthogonal_vectors

# Example with custom metric
metric = np.array([[2, 1, 0], [1, 2, 1], [0, 1, 2]])
inner_space = InnerProductSpace("real", metric)

v1 = np.array([1, 0, 0])
v2 = np.array([0, 1, 0])

print(f"Inner product: {inner_space.inner_product(v1, v2)}")
print(f"Angle: {np.degrees(inner_space.angle(v1, v2)):.2f}Â°")
```

## Advanced Matrix Theory

### Matrix Spaces and Operations

**Definition**: The set of mÃ—n matrices over field F forms a vector space Mâ‚˜â‚™(F) with:
- **Addition**: (A + B)áµ¢â±¼ = Aáµ¢â±¼ + Báµ¢â±¼
- **Scalar multiplication**: (cA)áµ¢â±¼ = cAáµ¢â±¼

**Matrix Multiplication Properties:**
- **Associativity**: (AB)C = A(BC)
- **Distributivity**: A(B + C) = AB + AC
- **Non-commutativity**: AB â‰  BA (in general)
- **Transpose properties**: (AB)áµ€ = Báµ€Aáµ€, (A + B)áµ€ = Aáµ€ + Báµ€

### Advanced Matrix Types

```python
class AdvancedMatrix:
    """Advanced matrix operations and analysis"""
    
    def __init__(self, matrix: np.ndarray):
        self.matrix = matrix
        self.m, self.n = matrix.shape
    
    def is_symmetric(self) -> bool:
        """Check if matrix is symmetric"""
        return np.allclose(self.matrix, self.matrix.T)
    
    def is_hermitian(self) -> bool:
        """Check if matrix is Hermitian (A = A*)"""
        return np.allclose(self.matrix, self.matrix.conj().T)
    
    def is_orthogonal(self) -> bool:
        """Check if matrix is orthogonal (Aáµ€A = I)"""
        if self.m != self.n:
            return False
        return np.allclose(self.matrix.T @ self.matrix, np.eye(self.m))
    
    def is_unitary(self) -> bool:
        """Check if matrix is unitary (A*A = I)"""
        if self.m != self.n:
            return False
        return np.allclose(self.matrix.conj().T @ self.matrix, np.eye(self.m))
    
    def condition_number(self) -> float:
        """Compute condition number Îº(A) = Ïƒâ‚/Ïƒâ‚™"""
        if self.m != self.n:
            raise ValueError("Condition number only defined for square matrices")
        
        singular_values = np.linalg.svd(self.matrix, compute_uv=False)
        return singular_values[0] / singular_values[-1]
    
    def trace(self) -> Union[float, complex]:
        """Compute matrix trace"""
        if self.m != self.n:
            raise ValueError("Trace only defined for square matrices")
        return np.trace(self.matrix)
    
    def frobenius_norm(self) -> float:
        """Compute Frobenius norm ||A||_F = âˆš(Î£|Aáµ¢â±¼|Â²)"""
        return np.sqrt(np.sum(np.abs(self.matrix)**2))
    
    def nuclear_norm(self) -> float:
        """Compute nuclear norm ||A||_* = Î£Ïƒáµ¢"""
        singular_values = np.linalg.svd(self.matrix, compute_uv=False)
        return np.sum(singular_values)

# Example usage
A = np.array([[1, 2], [2, 4]])
B = np.array([[0, 1], [1, 0]])

matrix_A = AdvancedMatrix(A)
matrix_B = AdvancedMatrix(B)

print(f"A is symmetric: {matrix_A.is_symmetric()}")
print(f"B is orthogonal: {matrix_B.is_orthogonal()}")
print(f"Condition number of A: {matrix_A.condition_number():.2f}")
print(f"Frobenius norm of B: {matrix_B.frobenius_norm():.2f}")
```

### Matrix Decompositions

**LU Decomposition**: A = LU where L is lower triangular and U is upper triangular.

**Cholesky Decomposition**: A = LLáµ€ where A is positive definite and L is lower triangular.

```python
def lu_decomposition(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """LU decomposition with partial pivoting"""
    n = A.shape[0]
    L = np.eye(n)
    U = A.copy()
    P = np.eye(n)
    
    for k in range(n-1):
        # Find pivot
        pivot_row = k + np.argmax(np.abs(U[k:, k]))
        
        if pivot_row != k:
            # Swap rows
            U[[k, pivot_row]] = U[[pivot_row, k]]
            P[[k, pivot_row]] = P[[pivot_row, k]]
            L[[k, pivot_row], :k] = L[[pivot_row, k], :k]
        
        # Eliminate column k
        for i in range(k+1, n):
            factor = U[i, k] / U[k, k]
            L[i, k] = factor
            U[i, k:] -= factor * U[k, k:]
    
    return L, U, P

def cholesky_decomposition(A: np.ndarray) -> np.ndarray:
    """Cholesky decomposition A = LLáµ€"""
    if not np.allclose(A, A.T):
        raise ValueError("Matrix must be symmetric")
    
    n = A.shape[0]
    L = np.zeros_like(A)
    
    for i in range(n):
        for j in range(i+1):
            if i == j:
                L[i, j] = np.sqrt(A[i, i] - np.sum(L[i, :j]**2))
            else:
                L[i, j] = (A[i, j] - np.sum(L[i, :j] * L[j, :j])) / L[j, j]
    
    return L

# Example
A = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]])
L_chol = cholesky_decomposition(A)
print("Cholesky decomposition:")
print(L_chol)
print(f"Verification: A = LLáµ€? {np.allclose(A, L_chol @ L_chol.T)}")
```

## Advanced Tensor Theory

### Tensor Definition and Properties

**Definition**: A **tensor** of order k is a multilinear map:
```
T : Vâ‚ Ã— Vâ‚‚ Ã— ... Ã— Vâ‚– â†’ F
```

**Tensor Properties:**
- **Multilinearity**: T(vâ‚, ..., aváµ¢ + bwáµ¢, ..., vâ‚–) = aT(vâ‚, ..., váµ¢, ..., vâ‚–) + bT(vâ‚, ..., wáµ¢, ..., vâ‚–)
- **Coordinate representation**: Táµ¢â‚áµ¢â‚‚...áµ¢â‚– = T(eáµ¢â‚, eáµ¢â‚‚, ..., eáµ¢â‚–)

### Tensor Operations

```python
class Tensor:
    """Advanced tensor operations and analysis"""
    
    def __init__(self, data: np.ndarray):
        self.data = data
        self.shape = data.shape
        self.order = len(data.shape)
    
    def reshape(self, new_shape: Tuple[int, ...]) -> 'Tensor':
        """Reshape tensor"""
        return Tensor(self.data.reshape(new_shape))
    
    def transpose(self, axes: Tuple[int, ...] = None) -> 'Tensor':
        """Transpose tensor dimensions"""
        if axes is None:
            axes = tuple(reversed(range(self.order)))
        return Tensor(self.data.transpose(axes))
    
    def contract(self, other: 'Tensor', indices: Tuple[Tuple[int, int], ...]) -> 'Tensor':
        """Tensor contraction (Einstein summation)"""
        # This is a simplified version - full implementation would be more complex
        return Tensor(np.tensordot(self.data, other.data, axes=indices))
    
    def outer_product(self, other: 'Tensor') -> 'Tensor':
        """Outer product of two tensors"""
        return Tensor(np.outer(self.data.flatten(), other.data.flatten()).reshape(
            self.shape + other.shape
        ))
    
    def mode_n_product(self, matrix: np.ndarray, mode: int) -> 'Tensor':
        """Mode-n product with matrix"""
        if mode >= self.order:
            raise ValueError(f"Mode {mode} out of range for tensor of order {self.order}")
        
        # Reshape tensor for matrix multiplication
        shape = list(self.shape)
        n = shape[mode]
        shape[mode] = -1
        
        # Reshape and multiply
        reshaped = self.data.reshape(shape)
        if mode == 0:
            result = matrix @ reshaped
        else:
            # Need to transpose to get mode to first dimension
            axes = list(range(self.order))
            axes[0], axes[mode] = axes[mode], axes[0]
            transposed = self.data.transpose(axes)
            reshaped = transposed.reshape(n, -1)
            result = matrix @ reshaped
            # Transpose back
            result = result.reshape([matrix.shape[0]] + [self.shape[i] for i in axes[1:]])
            axes_back = list(range(self.order))
            axes_back[0], axes_back[mode] = axes_back[mode], axes_back[0]
            result = result.transpose(axes_back)
        
        return Tensor(result)
    
    def svd_decomposition(self, mode: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
        """Mode-n SVD decomposition"""
        # Reshape tensor for SVD
        shape = list(self.shape)
        n = shape[mode]
        shape[mode] = -1
        
        reshaped = self.data.reshape(shape)
        if mode != 0:
            # Transpose to get mode to first dimension
            axes = list(range(self.order))
            axes[0], axes[mode] = axes[mode], axes[0]
            reshaped = self.data.transpose(axes).reshape(n, -1)
        
        U, S, Vt = np.linalg.svd(reshaped, full_matrices=False)
        return U, S, Vt

# Example: 3D tensor
tensor_data = np.random.rand(3, 4, 5)
tensor = Tensor(tensor_data)

print(f"Tensor shape: {tensor.shape}")
print(f"Tensor order: {tensor.order}")

# Mode-1 SVD
U, S, Vt = tensor.svd_decomposition(1)
print(f"Mode-1 SVD: U shape {U.shape}, S length {len(S)}, Vt shape {Vt.shape}")
```

### Tensor Networks and Contractions

**Tensor Network**: A graph where nodes represent tensors and edges represent contractions.

```python
class TensorNetwork:
    """Tensor network representation and operations"""
    
    def __init__(self):
        self.tensors = {}
        self.connections = {}
    
    def add_tensor(self, name: str, tensor: Tensor):
        """Add tensor to network"""
        self.tensors[name] = tensor
    
    def connect(self, tensor1: str, index1: int, tensor2: str, index2: int):
        """Connect two tensor indices"""
        if tensor1 not in self.connections:
            self.connections[tensor1] = {}
        if tensor2 not in self.connections:
            self.connections[tensor2] = {}
        
        self.connections[tensor1][index1] = (tensor2, index2)
        self.connections[tensor2][index2] = (tensor1, index1)
    
    def contract_network(self) -> Tensor:
        """Contract entire tensor network (simplified)"""
        # This is a simplified implementation
        # Real tensor network contraction is NP-hard and requires sophisticated algorithms
        
        if len(self.tensors) == 0:
            raise ValueError("No tensors in network")
        
        if len(self.tensors) == 1:
            return list(self.tensors.values())[0]
        
        # For simplicity, just contract pairs
        result = None
        for name, tensor in self.tensors.items():
            if result is None:
                result = tensor
            else:
                # Simple contraction (this is not the general case)
                result = result.outer_product(tensor)
        
        return result

# Example tensor network
network = TensorNetwork()

# Add tensors
tensor_A = Tensor(np.random.rand(2, 3))
tensor_B = Tensor(np.random.rand(3, 4))
tensor_C = Tensor(np.random.rand(4, 2))

network.add_tensor('A', tensor_A)
network.add_tensor('B', tensor_B)
network.add_tensor('C', tensor_C)

# Connect tensors (forming a cycle)
network.connect('A', 1, 'B', 0)
network.connect('B', 1, 'C', 0)
network.connect('C', 1, 'A', 0)

print("Tensor network created with 3 tensors")
```

## Advanced ML Context

### Neural Network Weight Analysis

```python
class AdvancedNeuralNetwork:
    """Advanced neural network with weight analysis"""
    
    def __init__(self, layer_sizes: List[int]):
        self.layer_sizes = layer_sizes
        self.weights = []
        self.biases = []
        
        # Initialize with advanced techniques
        for i in range(len(layer_sizes) - 1):
            # He initialization for ReLU
            std = np.sqrt(2.0 / layer_sizes[i])
            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * std
            b = np.zeros((1, layer_sizes[i+1]))
            
            self.weights.append(w)
            self.biases.append(b)
    
    def analyze_weights(self) -> dict:
        """Comprehensive weight analysis"""
        analysis = {}
        
        for i, w in enumerate(self.weights):
            layer_analysis = {}
            
            # Basic statistics
            layer_analysis['mean'] = np.mean(w)
            layer_analysis['std'] = np.std(w)
            layer_analysis['min'] = np.min(w)
            layer_analysis['max'] = np.max(w)
            
            # Matrix properties
            if w.shape[0] == w.shape[1]:
                eigenvals = np.linalg.eigvals(w)
                layer_analysis['eigenvalues'] = eigenvals
                layer_analysis['condition_number'] = np.max(np.abs(eigenvals)) / np.min(np.abs(eigenvals))
                layer_analysis['determinant'] = np.linalg.det(w)
            
            # SVD analysis
            U, S, Vt = np.linalg.svd(w, full_matrices=False)
            layer_analysis['singular_values'] = S
            layer_analysis['rank'] = np.sum(S > 1e-10)
            layer_analysis['effective_rank'] = np.sum(S > S[0] * 1e-6)
            
            # Norms
            layer_analysis['frobenius_norm'] = np.linalg.norm(w, 'fro')
            layer_analysis['spectral_norm'] = S[0]
            layer_analysis['nuclear_norm'] = np.sum(S)
            
            # Gradient flow analysis
            layer_analysis['gradient_scale'] = np.std(w) / np.mean(np.abs(w))
            
            analysis[f'layer_{i}'] = layer_analysis
        
        return analysis
    
    def weight_regularization(self, reg_type: str = 'l2', strength: float = 0.01):
        """Apply weight regularization"""
        if reg_type == 'l2':
            for w in self.weights:
                w *= (1 - strength)
        elif reg_type == 'l1':
            for w in self.weights:
                w -= strength * np.sign(w)
        elif reg_type == 'spectral':
            for w in self.weights:
                U, S, Vt = np.linalg.svd(w, full_matrices=False)
                S = np.clip(S, 0, 1/strength)
                w[:] = U @ np.diag(S) @ Vt

# Example usage
nn = AdvancedNeuralNetwork([784, 256, 128, 10])
analysis = nn.analyze_weights()

print("Weight Analysis Summary:")
for layer_name, layer_analysis in analysis.items():
    print(f"\n{layer_name}:")
    print(f"  Rank: {layer_analysis['rank']}/{layer_analysis['effective_rank']}")
    print(f"  Condition number: {layer_analysis['condition_number']:.2f}")
    print(f"  Spectral norm: {layer_analysis['spectral_norm']:.4f}")
```

### Advanced Data Representations

```python
class AdvancedDataRepresentation:
    """Advanced data representation using tensors"""
    
    def __init__(self, data_type: str):
        self.data_type = data_type
    
    def create_image_tensor(self, batch_size: int, height: int, width: int, channels: int) -> Tensor:
        """Create image tensor with proper structure"""
        # Simulate image data
        data = np.random.rand(batch_size, height, width, channels)
        return Tensor(data)
    
    def create_sequence_tensor(self, batch_size: int, seq_length: int, features: int) -> Tensor:
        """Create sequence tensor for NLP/RNN"""
        data = np.random.rand(batch_size, seq_length, features)
        return Tensor(data)
    
    def create_graph_tensor(self, num_nodes: int, num_edges: int, node_features: int) -> dict:
        """Create graph tensor representation"""
        # Node features
        node_tensor = Tensor(np.random.rand(num_nodes, node_features))
        
        # Adjacency matrix
        adj_matrix = np.zeros((num_nodes, num_nodes))
        # Add some random edges
        edge_indices = np.random.choice(num_nodes * num_nodes, num_edges, replace=False)
        for idx in edge_indices:
            i, j = idx // num_nodes, idx % num_nodes
            adj_matrix[i, j] = 1
        
        adj_tensor = Tensor(adj_matrix)
        
        return {
            'nodes': node_tensor,
            'adjacency': adj_tensor,
            'num_nodes': num_nodes,
            'num_edges': num_edges
        }
    
    def tensor_operations(self, tensor: Tensor) -> dict:
        """Perform advanced tensor operations"""
        operations = {}
        
        # Reshape operations
        operations['flatten'] = tensor.reshape((-1,))
        operations['transpose_01'] = tensor.transpose((1, 0, 2)) if tensor.order >= 3 else tensor
        
        # Mode operations
        if tensor.order >= 2:
            random_matrix = np.random.rand(tensor.shape[0], 10)
            operations['mode_0_product'] = tensor.mode_n_product(random_matrix, 0)
        
        # SVD analysis
        if tensor.order >= 2:
            U, S, Vt = tensor.svd_decomposition(0)
            operations['svd_mode_0'] = {'U': U, 'S': S, 'Vt': Vt}
        
        return operations

# Example usage
data_rep = AdvancedDataRepresentation("mixed")

# Create different tensor types
image_tensor = data_rep.create_image_tensor(32, 224, 224, 3)
sequence_tensor = data_rep.create_sequence_tensor(64, 100, 512)
graph_data = data_rep.create_graph_tensor(100, 500, 64)

print(f"Image tensor shape: {image_tensor.shape}")
print(f"Sequence tensor shape: {sequence_tensor.shape}")
print(f"Graph: {graph_data['num_nodes']} nodes, {graph_data['num_edges']} edges")

# Perform operations
image_ops = data_rep.tensor_operations(image_tensor)
print(f"Image tensor operations: {list(image_ops.keys())}")
```

## Advanced Practice Problems

### Theoretical Challenges

1. **Vector Space Proofs**:
   - Prove that the intersection of two subspaces is a subspace
   - Show that the span of a set of vectors is the smallest subspace containing them
   - Prove the rank-nullity theorem

2. **Matrix Analysis**:
   - Prove that similar matrices have the same eigenvalues
   - Show that the determinant of a product equals the product of determinants
   - Prove the Cayley-Hamilton theorem

3. **Tensor Theory**:
   - Prove that tensor contraction is associative
   - Show that the rank of a tensor is invariant under coordinate changes
   - Prove the tensor decomposition theorem

### Implementation Challenges

1. **Advanced Algorithms**:
   - Implement Strassen's matrix multiplication algorithm
   - Create a sparse matrix class with optimized operations
   - Build a tensor network contraction optimizer

2. **Performance Optimization**:
   - Implement GPU-accelerated tensor operations using CuPy
   - Create a distributed matrix factorization system
   - Build a memory-efficient transformer attention mechanism

3. **Research Applications**:
   - Implement geometric deep learning on manifolds
   - Create a quantum-inspired tensor network
   - Build an adversarial robust neural network

## Next Steps

In the next tutorial, we'll explore advanced matrix multiplication techniques, including:
- **Strassen's algorithm** and fast matrix multiplication
- **Block matrix operations** and cache optimization
- **Sparse matrix algorithms** and graph operations
- **Parallel and distributed** matrix computations
- **GPU acceleration** and memory optimization
- **Matrix chain multiplication** and dynamic programming

This foundation will enable you to understand and implement the most advanced machine learning algorithms with mathematical rigor and computational efficiency.
