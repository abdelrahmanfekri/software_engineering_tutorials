# Module 2: Convolutional Neural Networks and Architectures

**AI Content Generation Prompt:**
Create a comprehensive deep dive into CNNs and modern architectures covering:

1. **CNN Fundamentals**
   - Convolution operation (mathematical definition, implementation)
   - Receptive fields and feature maps
   - Padding (valid, same, causal)
   - Stride and dilation
   - Parameter sharing and local connectivity
   - Backpropagation through convolutions
   - PyTorch/TensorFlow implementation from scratch

2. **Building Blocks**
   - Pooling layers (max, average, global, adaptive)
   - Batch normalization, layer normalization, group normalization
   - Activation functions (ReLU, LeakyReLU, GELU, Swish)
   - Dropout and regularization techniques
   - Skip connections and residual blocks
   - Depthwise separable convolutions
   - Code implementations for each

3. **Classic CNN Architectures**
   - LeNet-5 (historical context and architecture)
   - AlexNet (breakthrough in ImageNet)
   - VGGNet (depth and simplicity)
   - GoogLeNet/Inception (multi-scale features)
   - ResNet (residual connections, solving degradation)
   - DenseNet (dense connections)
   - Implementation and training examples for each
   - Architectural evolution and design principles

4. **Modern CNN Architectures**
   - MobileNet family (efficient mobile architectures)
   - EfficientNet (compound scaling)
   - ResNeXt (group convolutions)
   - ConvNeXt (modernizing CNNs)
   - NFNet (normalized-free networks)
   - RegNet (design space exploration)
   - Comparative analysis and use cases

5. **Vision Transformers**
   - Transformer architecture adapted for vision
   - Vision Transformer (ViT) architecture
   - Patch embeddings and positional encoding
   - Self-attention for images
   - Hybrid architectures (CNN + Transformer)
   - Swin Transformer (shifted windows)
   - DeiT (data-efficient image transformers)
   - Implementation details and code

6. **Training CNNs**
   - Data augmentation techniques (geometric, color, cutout, mixup)
   - Transfer learning and fine-tuning strategies
   - Learning rate schedules
   - Optimization tricks (label smoothing, gradient clipping)
   - Handling class imbalance
   - Multi-GPU training
   - Complete training pipeline implementation

7. **Architecture Design Principles**
   - Network depth vs width trade-offs
   - Efficient architecture design
   - Neural Architecture Search (NAS) basics
   - AutoML for vision
   - Design patterns and best practices

Include:
- Detailed architecture diagrams
- Mathematical formulations
- Full PyTorch implementations
- Training code for ImageNet-style classification
- Comparative benchmarks (accuracy, speed, parameters)
- Ablation studies
- Practical tips and common pitfalls
- 15-20 exercises
- Assessment questions covering theory and implementation

Target: 1200-1500 lines with extensive code examples and visualizations.

